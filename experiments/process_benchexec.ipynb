{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f61ecc3-8076-46de-ac6c-739412b48c6f",
   "metadata": {},
   "source": [
    "# Process BenchExec Experiments\n",
    "\n",
    "This notebook post-processes the results obtained from Benchexec to generate two tables:\n",
    "\n",
    "1. A flat table of stats, that can be used for further analysis and plotting.\n",
    "2. A coverage table per domain and solver, typically reported in papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a61c4-d7a1-4b58-977d-c2bc468b61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# sys.dont_write_bytecode = True  # prevent creation of .pyc files\n",
    "\n",
    "CSV_FOLDER = \"stats/july24-redo-benchexec/\"\n",
    "NAME_EXPERIMENT = \"cfond\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0cd1bd",
   "metadata": {},
   "source": [
    "## 1. Flatten Benchexec CSV tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a27abb",
   "metadata": {},
   "source": [
    "Collect all CSV benchexec table result files under `CSV_FOLDER` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CSV files found in folder {CSV_FOLDER}:\")\n",
    "files = glob.glob(os.path.join(CSV_FOLDER, \"**\", \"benchmark-*.csv\"), recursive=True)\n",
    "\n",
    "files += glob.glob(os.path.join(CSV_FOLDER, \"**\", \"results.*.table.csv\"), recursive=True)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157c50e",
   "metadata": {},
   "source": [
    "Load all CSV data into a single dataframe. Note that each CSV file may include results for many _run sets_, with each having its own (set of) columns.\n",
    "\n",
    "Each row is the result of a _task_ in the experiment, and every run set has its columns stats for such task. Presumably sets of columns share the same schema.\n",
    "\n",
    "We then need reshape this structure to have just one set of columns and a new column identifying the run set. So each original row will be expanded into many rows, one per run set.\n",
    "\n",
    "The first three lines contain header:\n",
    "\n",
    "1. First line contains the _tool_ used. It starts with `tool` followed by the name of the tool repeated multiple times (to match no. of columns).\n",
    "2. Second line contains the _runs_ of the experiment. It starts with `run set` and then sets of columns with the name of the runs.\n",
    "3. Third line contains the _stats_ column names repeated per run set. First column is for name of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ecb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAME_COLS = {\"benchmarks/benchexe/tasks/\": \"id\", \"cputime (s)\": \"cputime\", \"walltime (s)\": \"walltime\", \"memory (MB)\": \"memory_mb\"}\n",
    "\n",
    "def get_meta_csv(file):\n",
    "    \"\"\"Given a benchexec CSV file, extract the runs (e.g., prp, prp_inv) and how many columns per run\"\"\"\n",
    "    with open(file, \"r\") as f:\n",
    "        # first line contains the tool used (repeated one per column needed); e.g.,  PP-FOND\n",
    "        tools_header = f.readline().split()[1:]\n",
    "        # second line contains the run/solvers used in the experiment (e.g., prp, prp_inv) and starts with \"run set\" to be ignored\n",
    "        runs_header = f.readline().split()[2:]\n",
    "\n",
    "    runs = set(runs_header)\n",
    "\n",
    "    no_cols = int(len(runs_header) / len(runs))\n",
    "\n",
    "    return runs, no_cols\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    runs, no_cols = get_meta_csv(f)\n",
    "    print(f\"Runs in file {f}: {runs} with {no_cols} stat columns\")\n",
    "\n",
    "    # go over each set of run columns (a csv file may contain many runs, each with the same columns)\n",
    "    for k, r in enumerate(list(runs)):\n",
    "        col_idx = [0] + list(range(k*no_cols + 1, k*no_cols + no_cols + 1))\n",
    "        print(f\"\\t Extracting run '{r}' in columns: {col_idx}\")\n",
    "\n",
    "        # read the CSV file from line 3+ (line 3 is header)\n",
    "        df = pd.read_csv(f, delimiter=\"\\t\", skiprows=2, usecols=col_idx)\n",
    "        df.rename(columns=lambda x: x.split('.')[0], inplace=True)\n",
    "\n",
    "        df.columns.values[0] = \"task\"\n",
    "        # df.rename(columns={df.columns[1]: \"task\"})\n",
    "\n",
    "        # populate column run with name of run-solver r\n",
    "        df.insert(1, 'run', r)\n",
    "        dfs.append(df)\n",
    "\n",
    "df_csv = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "df_csv.rename(columns=RENAME_COLS, inplace=True)\n",
    "\n",
    "print(\"runs found:\", df_csv[\"run\"].unique())\n",
    "# df.set_index(\"task\", inplace=True)\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver/runs found\n",
    "print(\"Solvers/run found:\", df_csv['run'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a08eb",
   "metadata": {},
   "source": [
    "We next **enrich** the dataframe with derived columns:\n",
    "\n",
    "* domain\n",
    "* instance\n",
    "* solver\n",
    "* solved (boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78deaee6-26c0-47a9-adab-040f390599b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_benchmark_labels(task_name):\n",
    "    \"\"\"From the task description name (e.g., acrobatics_01.yml), extract the benchmark labels, like domain, instance\"\"\"\n",
    "    regex = r\"(.+)_([0-9]+)\\.yml\"\n",
    "\n",
    "    match = re.match(regex, task_name)\n",
    "    if match:\n",
    "        # print(match.groups())\n",
    "        domain = match.group(1)\n",
    "        instance = match.group(2)\n",
    "    else:\n",
    "        print(\"Problem extracting labels from task name\", task_name)\n",
    "    return domain, instance\n",
    "\n",
    "df = df_csv.copy()\n",
    "\n",
    "# 1 - split task name into domain and instances\n",
    "df[\"benchmark\"] = df.reset_index()[\"task\"].map(get_benchmark_labels).values\n",
    "df[\"domain\"] = df[\"benchmark\"].str.get(0)\n",
    "df[\"instance\"] = df[\"benchmark\"].str.get(1)\n",
    "df.drop(columns=[\"benchmark\"], inplace=True)\n",
    "\n",
    "# 2 - map status from benchexec to integers status\n",
    "map_status = {\n",
    "    \"true\": 1,\n",
    "    \"false\": 0,\n",
    "    \"True\": 1,\n",
    "    \"False\": 0,\n",
    "    False: 0,\n",
    "    True: 1,\n",
    "    \"OUT OF MEMORY (false)\": -2,\n",
    "    \"TIMEOUT (false)\": -1,\n",
    "    \"TIMEOUT (true)\": 1,\n",
    "}\n",
    "df[\"status2\"] = df[\"status\"].map(map_status)\n",
    "\n",
    "missing_mapping = df[df[\"status2\"].isnull()].shape[0]\n",
    "if missing_mapping > 0:\n",
    "    missing_status = [x for x in df[\"status\"].unique() if x not in map_status.keys()]\n",
    "    print(f\"WARNING: {missing_mapping} status values not mapped:\", missing_status)\n",
    "    print(df[df[\"status2\"].isnull()])\n",
    "\n",
    "df[\"status\"] = df[\"status2\"]\n",
    "df.drop(columns=[\"status2\"], inplace=True)\n",
    "\n",
    "# 3 - define Boolean column solved to flag if solved or not based on status\n",
    "df.insert(3, \"solved\", df[\"status\"].apply(lambda x: True if x == 1 else False))\n",
    "\n",
    "\n",
    "# 4 - extract solver from run name\n",
    "map_solver = {\n",
    "    \"prp.FOND\": \"prp\",\n",
    "    \"paladinus.FOND\": \"paladinus\",\n",
    "    \"fondsat-glucose.FOND\": \"fondsat-glucose\",\n",
    "    \"fondsat-minisat.FOND\": \"fondsat-minisat\",\n",
    "    \"cfondasp1-reg.FOND\" : \"cfondasp1-reg\",\n",
    "    \"cfondasp2-reg.FOND\" : \"cfondasp2-reg\",\n",
    "    \"cfondasp1-fsat\" : \"asp1-fsat\",\n",
    "    \"cfondasp2-fsat\" : \"asp2-fsat\",\n",
    "}\n",
    "df[\"solver\"] = df[\"run\"].map(map_solver)\n",
    "\n",
    "# sanity check status\n",
    "# df.query(\"status not in [-1,0,-2,1]\")\n",
    "# df.status = df.status.astype(int) # convert to int\n",
    "# df.loc[df.status == \"OUT OF MEMORY (false)\"]\n",
    "# df.loc[df.status == -1]\n",
    "\n",
    "# df.dtypes\n",
    "\n",
    "# note that status should be integer; if float it is bc there must be NaN value!\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fa16b-fcec-42b8-ba4e-d9668cae4c3d",
   "metadata": {},
   "source": [
    "Finally, save all results into a complete CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f5437-6477-40b8-b6df-6dcde7bd36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(CSV_FOLDER, f\"{NAME_EXPERIMENT}_benchexec_stats.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3940d3",
   "metadata": {},
   "source": [
    "## 2. Coverage Analysis and Table\n",
    "\n",
    "We now generate **coverage** tables, as they often apper in papers. Basically we compute per benchmark set, domain, and APP type sub-domain, and each solver-run:\n",
    "\n",
    "- **Coverage:** % of solved instances solved by the solver-run; and\n",
    "- **Stat metrics:** mean on time, memory usage, and policy size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69da6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19787b9",
   "metadata": {},
   "source": [
    "Calculate % ratio per set/domain/sub_domain/run-solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby([\"domain\", \"solver\"])\n",
    "\n",
    "#   df_grouped.sum()[[\"solved\"]] = sum all the True instances (sum over bool = number of True)\n",
    "#   df_grouped.count()[[\"solved\"]] = number of rows in solved column (includes True and Talse values)\n",
    "df_coverage = df_grouped.sum()[[\"solved\"]] / df_grouped.count()[[\"solved\"]]\n",
    "df_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63285049",
   "metadata": {},
   "source": [
    "Calculate mean metric (for CPU time, memory, and policy size) across the solved instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"domain\", \"solver\", \"cputime\", \"memory_mb\", \"policy_size\"]\n",
    "df_solved = df.query(\"solved == True\")[columns]\n",
    "\n",
    "df_solved_grouped = df_solved.groupby([\"domain\", \"solver\"])\n",
    "df_metrics = df_solved_grouped.mean()\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e256f8",
   "metadata": {},
   "source": [
    "Put together **Coverage** and **Metrics** tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc03c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {\n",
    "    \"solved\": \"cov\",\n",
    "    \"cputime\": \"time\",\n",
    "    \"memory_mb\": \"mem\",\n",
    "    \"policy_size\": \"size\",\n",
    "}\n",
    "\n",
    "df_stats = df_coverage.join(df_metrics, how=\"inner\")\n",
    "df_stats.rename(columns=column_names, inplace=True)\n",
    "\n",
    "df_stats = df_stats.reset_index()\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c66453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_pivot = df_stats.pivot(\n",
    "    index=[\"domain\"],\n",
    "    values=[\"cov\", \"time\", \"mem\", \"size\"],\n",
    "    columns=\"solver\",\n",
    ")\n",
    "df_stats_pivot.reset_index(\n",
    "    inplace=True\n",
    ")  # unfold multi-index into columns (create integer index)\n",
    "df_stats_pivot.columns = [\n",
    "    \"_\".join(tup).rstrip(\"_\") for tup in df_stats_pivot.columns.values\n",
    "]\n",
    "\n",
    "# flat index, but multi-column: 1. coverage / time / policy size and 2. each solver/run\n",
    "df_stats_pivot = df_stats_pivot.round(2)\n",
    "\n",
    "df_stats_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086112ae",
   "metadata": {},
   "source": [
    "Save it to the file, this can be used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_pivot.to_csv(os.path.join(CSV_FOLDER, f\"{NAME_EXPERIMENT}_coverage_table.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
